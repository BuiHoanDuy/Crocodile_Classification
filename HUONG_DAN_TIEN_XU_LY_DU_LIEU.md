# üìö H∆∞·ªõng D·∫´n Chi Ti·∫øt: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu v√† X√¢y D·ª±ng M√¥ H√¨nh D·ª± ƒêo√°n T√¨nh Tr·∫°ng B·∫£o T·ªìn C√° S·∫•u

## üìã M·ª•c L·ª•c

1. [T·ªïng quan](#t·ªïng-quan)
2. [Ph·∫ßn 1: Khai b√°o th∆∞ vi·ªán v√† ƒë·ªçc d·ªØ li·ªáu](#ph·∫ßn-1-khai-b√°o-th∆∞-vi·ªán-v√†-ƒë·ªçc-d·ªØ-li·ªáu)
3. [Ph·∫ßn 2: L√†m s·∫°ch d·ªØ li·ªáu](#ph·∫ßn-2-l√†m-s·∫°ch-d·ªØ-li·ªáu)
4. [Ph·∫ßn 3: X·ª≠ l√Ω Outlier (Capping)](#ph·∫ßn-3-x·ª≠-l√Ω-outlier-capping)
5. [Ph·∫ßn 4: M√£ h√≥a v√† Chu·∫©n h√≥a](#ph·∫ßn-4-m√£-h√≥a-v√†-chu·∫©n-h√≥a)
6. [Ph·∫ßn 5: T√≠ch h·ª£p d·ªØ li·ªáu](#ph·∫ßn-5-t√≠ch-h·ª£p-d·ªØ-li·ªáu)
7. [Ph·∫ßn 6: Ph√¢n t√≠ch t∆∞∆°ng quan](#ph·∫ßn-6-ph√¢n-t√≠ch-t∆∞∆°ng-quan)
8. [Ph·∫ßn 7: Chu·∫©n b·ªã d·ªØ li·ªáu Train/Test](#ph·∫ßn-7-chu·∫©n-b·ªã-d·ªØ-li·ªáu-traintest)
9. [Ph·∫ßn 8: Tr·ª±c quan h√≥a d·ªØ li·ªáu](#ph·∫ßn-8-tr·ª±c-quan-h√≥a-d·ªØ-li·ªáu)
10. [Ph·∫ßn 9: X√¢y d·ª±ng v√† ƒë√°nh gi√° m√¥ h√¨nh](#ph·∫ßn-9-x√¢y-d·ª±ng-v√†-ƒë√°nh-gi√°-m√¥-h√¨nh)
11. [Ph·∫ßn 10: So s√°nh c√°c m√¥ h√¨nh](#ph·∫ßn-10-so-s√°nh-c√°c-m√¥-h√¨nh)
12. [Ph·∫ßn 11: Demo ·ª©ng d·ª•ng](#ph·∫ßn-11-demo-·ª©ng-d·ª•ng)

---

## üéØ T·ªïng quan

Notebook n√†y th·ª±c hi·ªán quy tr√¨nh ho√†n ch·ªânh t·ª´ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫øn x√¢y d·ª±ng v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh machine learning ƒë·ªÉ d·ª± ƒëo√°n t√¨nh tr·∫°ng b·∫£o t·ªìn c·ªßa c√° s·∫•u. D·ªØ li·ªáu bao g·ªìm c√°c th√¥ng tin v·ªÅ chi·ªÅu d√†i, c√¢n n·∫∑ng, nh√≥m tu·ªïi, gi·ªõi t√≠nh, qu·ªëc gia, m√¥i tr∆∞·ªùng s·ªëng v√† khu v·ª±c ƒë·ªãa l√Ω.

### M·ª•c ti√™u:
- Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu v√† chu·∫©n h√≥a
- X√¢y d·ª±ng c√°c m√¥ h√¨nh ph√¢n lo·∫°i: Decision Tree (J48), Naive Bayes
- √Åp d·ª•ng K-Means clustering ƒë·ªÉ ph√¢n c·ª•m d·ªØ li·ªáu
- So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh
- Demo ·ª©ng d·ª•ng th·ª±c t·∫ø

---

## üì¶ Ph·∫ßn 1: Khai b√°o th∆∞ vi·ªán v√† ƒë·ªçc d·ªØ li·ªáu

### Code:
```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

df = pd.read_csv('crocodile_dataset.csv')
```

### Gi·∫£i th√≠ch:

**Th∆∞ vi·ªán ƒë∆∞·ª£c s·ª≠ d·ª•ng:**
- **pandas**: ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng (DataFrame)
- **numpy**: T√≠nh to√°n s·ªë h·ªçc v√† x·ª≠ l√Ω m·∫£ng
- **seaborn & matplotlib**: V·∫Ω bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a d·ªØ li·ªáu
- **sklearn.preprocessing**: M√£ h√≥a v√† chu·∫©n h√≥a d·ªØ li·ªáu
- **sklearn.model_selection**: Chia d·ªØ li·ªáu train/test

**ƒê·ªçc d·ªØ li·ªáu:**
- S·ª≠ d·ª•ng `pd.read_csv()` ƒë·ªÉ ƒë·ªçc file CSV v√†o DataFrame
- File CSV ch·ª©a th√¥ng tin v·ªÅ c√°c quan s√°t c√° s·∫•u v·ªõi c√°c c·ªôt nh∆∞:
  - Observation ID, Common Name, Scientific Name
  - Observed Length (m), Observed Weight (kg)
  - Age Class, Sex, Country/Region, Habitat Type
  - Conservation Status (bi·∫øn m·ª•c ti√™u)

**L∆∞u √Ω:**
- S·ª≠ d·ª•ng `try-except` ƒë·ªÉ x·ª≠ l√Ω l·ªói n·∫øu file kh√¥ng t·ªìn t·∫°i
- `df.head(3)` hi·ªÉn th·ªã 3 d√≤ng ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra d·ªØ li·ªáu

---

## üßπ Ph·∫ßn 2: L√†m s·∫°ch d·ªØ li·ªáu

### Code:
```python
# X·ª≠ l√Ω gi√° tr·ªã 'Unknown' trong c·ªôt Sex
if 'Sex' in df.columns:
    mask = df['Sex'].str.lower().isin(['unknown', 'unkown'])
    random_sex = np.random.choice(['Male', 'Female'], size=mask.sum())
    df.loc[mask, 'Sex'] = random_sex

# Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
cols_to_drop = ['Observation ID', 'Observer Name', 'Notes', 
                'Date of Observation', 'Common Name', 
                'Scientific Name', 'Family', 'Genus']
df_clean = df.drop(columns=cols_to_drop, errors='ignore')
```

### Gi·∫£i th√≠ch:

#### 2.1. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu trong c·ªôt Sex

**V·∫•n ƒë·ªÅ:** M·ªôt s·ªë d√≤ng c√≥ gi√° tr·ªã 'Unknown' ho·∫∑c 'Unkown' (l·ªói ch√≠nh t·∫£) trong c·ªôt gi·ªõi t√≠nh.

**Gi·∫£i ph√°p:**
- T√¨m t·∫•t c·∫£ c√°c d√≤ng c√≥ gi√° tr·ªã 'unknown' ho·∫∑c 'unkown' (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
- Random g√°n ng·∫´u nhi√™n 'Male' ho·∫∑c 'Female' cho c√°c gi√° tr·ªã n√†y
- S·ª≠ d·ª•ng `np.random.choice()` ƒë·ªÉ ƒë·∫£m b·∫£o ph√¢n ph·ªëi ng·∫´u nhi√™n

**T·∫°i sao l√†m v·∫≠y?**
- Gi·ªØ l·∫°i d·ªØ li·ªáu thay v√¨ x√≥a (kh√¥ng m·∫•t m·∫´u)
- Random g√°n gi√∫p tr√°nh bias trong d·ªØ li·ªáu
- Gi·ªõi t√≠nh l√† bi·∫øn quan tr·ªçng trong ph√¢n lo·∫°i

#### 2.2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt

**C√°c c·ªôt b·ªã lo·∫°i b·ªè:**
- `Observation ID`: ID quan s√°t (kh√¥ng c√≥ gi√° tr·ªã d·ª± ƒëo√°n)
- `Observer Name`: T√™n ng∆∞·ªùi quan s√°t (kh√¥ng li√™n quan)
- `Notes`: Ghi ch√∫ (d·ªØ li·ªáu kh√¥ng c·∫•u tr√∫c)
- `Date of Observation`: Ng√†y quan s√°t (kh√¥ng s·ª≠ d·ª•ng)
- `Common Name`, `Scientific Name`, `Family`, `Genus`: Th√¥ng tin ph√¢n lo·∫°i sinh h·ªçc (c√≥ th·ªÉ g√¢y r√≤ r·ªâ d·ªØ li·ªáu)

**T·∫°i sao lo·∫°i b·ªè?**
- **Tr√°nh r√≤ r·ªâ d·ªØ li·ªáu (Data Leakage)**: C√°c c·ªôt nh∆∞ Scientific Name c√≥ th·ªÉ ch·ª©a th√¥ng tin v·ªÅ Conservation Status
- **Gi·∫£m ƒë·ªô ph·ª©c t·∫°p**: Lo·∫°i b·ªè c√°c bi·∫øn kh√¥ng c√≥ gi√° tr·ªã d·ª± ƒëo√°n
- **TƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω**: √çt c·ªôt h∆°n = t√≠nh to√°n nhanh h∆°n

**K·∫øt qu·∫£:** DataFrame c√≤n l·∫°i 7 c·ªôt quan tr·ªçng:
- Observed Length (m)
- Observed Weight (kg)
- Age Class
- Sex
- Country/Region
- Habitat Type
- Conservation Status

---

## üìä Ph·∫ßn 3: X·ª≠ l√Ω Outlier (Capping)

### Code:
```python
def cap_outliers(df, column):
    Q1 = df[column].quantile(0.25)  # T·ª© ph√¢n v·ªã th·ª© nh·∫•t (25%)
    Q3 = df[column].quantile(0.75)  # T·ª© ph√¢n v·ªã th·ª© ba (75%)
    IQR = Q3 - Q1                    # Kho·∫£ng t·ª© ph√¢n v·ªã
    lower = Q1 - 1.5 * IQR           # Gi·ªõi h·∫°n d∆∞·ªõi
    upper = Q3 + 1.5 * IQR           # Gi·ªõi h·∫°n tr√™n
    
    # Capping: G√°n gi√° tr·ªã ngo√†i kho·∫£ng v·ªÅ gi·ªõi h·∫°n
    df[column] = np.where(df[column] < lower, lower,
                          np.where(df[column] > upper, upper, df[column]))
    return df

df_clean = cap_outliers(df_clean, 'Observed Length (m)')
df_clean = cap_outliers(df_clean, 'Observed Weight (kg)')
```

### Gi·∫£i th√≠ch:

#### 3.1. Ph∆∞∆°ng ph√°p IQR (Interquartile Range)

**IQR l√† g√¨?**
- IQR = Q3 - Q1 (kho·∫£ng c√°ch gi·ªØa t·ª© ph√¢n v·ªã th·ª© ba v√† th·ª© nh·∫•t)
- Q1: Gi√° tr·ªã t·∫°i v·ªã tr√≠ 25% c·ªßa d·ªØ li·ªáu
- Q3: Gi√° tr·ªã t·∫°i v·ªã tr√≠ 75% c·ªßa d·ªØ li·ªáu

**C√¥ng th·ª©c ph√°t hi·ªán outlier:**
- **Outlier d∆∞·ªõi**: Gi√° tr·ªã < Q1 - 1.5 √ó IQR
- **Outlier tr√™n**: Gi√° tr·ªã > Q3 + 1.5 √ó IQR

**V√≠ d·ª•:**
```
N·∫øu Q1 = 1.64m, Q3 = 3.01m
IQR = 3.01 - 1.64 = 1.37m
Lower bound = 1.64 - 1.5 √ó 1.37 = -0.415m
Upper bound = 3.01 + 1.5 √ó 1.37 = 5.065m
```

#### 3.2. Ph∆∞∆°ng ph√°p Capping

**Capping l√† g√¨?**
- Thay v√¨ x√≥a outlier, ta "gi·ªõi h·∫°n" gi√° tr·ªã v·ªÅ bi√™n
- Gi√° tr·ªã < lower bound ‚Üí g√°n = lower bound
- Gi√° tr·ªã > upper bound ‚Üí g√°n = upper bound

**∆Øu ƒëi·ªÉm c·ªßa Capping:**
- ‚úÖ Gi·ªØ l·∫°i t·∫•t c·∫£ m·∫´u (kh√¥ng m·∫•t d·ªØ li·ªáu)
- ‚úÖ Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa outlier ƒë·∫øn model
- ‚úÖ Ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ th·ªÉ c√≥ gi√° tr·ªã c·ª±c ƒëoan h·ª£p l·ªá

**So s√°nh v·ªõi ph∆∞∆°ng ph√°p kh√°c:**
- **X√≥a outlier**: M·∫•t d·ªØ li·ªáu, c√≥ th·ªÉ l√†m gi·∫£m s·ªë l∆∞·ª£ng m·∫´u
- **Winsorization**: T∆∞∆°ng t·ª± capping nh∆∞ng c√≥ th·ªÉ gi·ªØ nhi·ªÅu outlier h∆°n
- **Z-score**: D·ª±a tr√™n ƒë·ªô l·ªách chu·∫©n, nh·∫°y c·∫£m v·ªõi ph√¢n ph·ªëi kh√¥ng chu·∫©n

#### 3.3. Tr·ª±c quan h√≥a v·ªõi Boxplot

```python
sns.boxplot(y=df_clean['Observed Length (m)'])
```

**Boxplot hi·ªÉn th·ªã:**
- Q1, Q2 (median), Q3
- Whiskers: Gi·ªõi h·∫°n tr√™n/d∆∞·ªõi (th∆∞·ªùng l√† Q1-1.5√óIQR v√† Q3+1.5√óIQR)
- Outliers: C√°c ƒëi·ªÉm n·∫±m ngo√†i whiskers

---

## üî¢ Ph·∫ßn 4: M√£ h√≥a v√† Chu·∫©n h√≥a

### Code:
```python
# M√£ h√≥a bi·∫øn m·ª•c ti√™u
target_le = LabelEncoder()
df_clean['Conservation Status'] = target_le.fit_transform(df_clean['Conservation Status'])

# M√£ h√≥a Age Class
age_mapping = {'Hatchling': 0, 'Juvenile': 1, 'Subadult': 2, 'Adult': 3}
df_clean['Age Class'] = df_clean['Age Class'].map(age_mapping)

# M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i kh√°c
cat_cols = ['Sex', 'Country/Region', 'Habitat Type']
le = LabelEncoder()
for col in cat_cols:
    df_clean[col] = le.fit_transform(df_clean[col].astype(str))

# Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë
scaler = StandardScaler()
num_cols = ['Observed Length (m)', 'Observed Weight (kg)']
df_clean[num_cols] = scaler.fit_transform(df_clean[num_cols])
```

### Gi·∫£i th√≠ch:

#### 4.1. M√£ h√≥a bi·∫øn m·ª•c ti√™u (Target Encoding)

**LabelEncoder:**
- Chuy·ªÉn ƒë·ªïi c√°c nh√£n text th√†nh s·ªë nguy√™n
- V√≠ d·ª•: 'Critically Endangered' ‚Üí 0, 'Data Deficient' ‚Üí 1, ...

**Mapping nh√£n:**
```
'Critically Endangered': 0
'Data Deficient': 1
'Endangered': 2
'Least Concern': 3
'Vulnerable': 4
```

**L∆∞u √Ω:** 
- C·∫ßn l∆∞u mapping ƒë·ªÉ gi·∫£i m√£ l·∫°i sau khi d·ª± ƒëo√°n
- Th·ª© t·ª± m√£ h√≥a quan tr·ªçng cho m·ªôt s·ªë thu·∫≠t to√°n

#### 4.2. M√£ h√≥a Age Class

**Ordinal Encoding:**
- S·ª≠ d·ª•ng mapping th·ªß c√¥ng v√¨ Age Class c√≥ th·ª© t·ª± t·ª± nhi√™n
- Hatchling (0) < Juvenile (1) < Subadult (2) < Adult (3)

**T·∫°i sao kh√¥ng d√πng LabelEncoder?**
- LabelEncoder kh√¥ng ƒë·∫£m b·∫£o th·ª© t·ª±
- Mapping th·ªß c√¥ng gi·ªØ ƒë∆∞·ª£c √Ω nghƒ©a th·ª© t·ª± c·ªßa d·ªØ li·ªáu

#### 4.3. M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i kh√°c

**LabelEncoder cho Sex, Country/Region, Habitat Type:**
- M·ªói gi√° tr·ªã duy nh·∫•t ƒë∆∞·ª£c g√°n m·ªôt s·ªë nguy√™n
- V√≠ d·ª•: 'Male' ‚Üí 0, 'Female' ‚Üí 1
- 'Vietnam' ‚Üí 0, 'Thailand' ‚Üí 1, ...

**L∆∞u √Ω:**
- C·∫ßn fit ri√™ng cho m·ªói c·ªôt (kh√¥ng d√πng chung encoder)
- S·ª≠ d·ª•ng `astype(str)` ƒë·ªÉ ƒë·∫£m b·∫£o x·ª≠ l√Ω ƒë√∫ng c√°c gi√° tr·ªã ƒë·∫∑c bi·ªát

#### 4.4. Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë (Standardization)

**StandardScaler:**
- Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu v·ªÅ ph√¢n ph·ªëi chu·∫©n v·ªõi mean=0, std=1
- C√¥ng th·ª©c: `z = (x - mean) / std`

**V√≠ d·ª•:**
```
Tr∆∞·ªõc chu·∫©n h√≥a:
- Mean Length = 2.42m, Std = 1.10m
- Mean Weight = 155.77kg, Std = 175.19kg

Sau chu·∫©n h√≥a:
- Mean Length = 0, Std = 1
- Mean Weight = 0, Std = 1
```

**T·∫°i sao c·∫ßn chu·∫©n h√≥a?**
- ‚úÖ C√°c thu·∫≠t to√°n d·ª±a tr√™n kho·∫£ng c√°ch (K-Means, SVM) ho·∫°t ƒë·ªông t·ªët h∆°n
- ‚úÖ Gradient descent h·ªôi t·ª• nhanh h∆°n
- ‚úÖ Tr√°nh bias do scale kh√°c nhau gi·ªØa c√°c bi·∫øn
- ‚úÖ M·ªôt s·ªë thu·∫≠t to√°n y√™u c·∫ßu d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a

**So s√°nh v·ªõi Normalization (Min-Max Scaling):**
- **StandardScaler**: Mean=0, Std=1 (ph√π h·ª£p khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi chu·∫©n)
- **MinMaxScaler**: Scale v·ªÅ [0, 1] (ph√π h·ª£p khi c·∫ßn gi·ªØ nguy√™n ph√¢n ph·ªëi)

---

## üåç Ph·∫ßn 5: T√≠ch h·ª£p d·ªØ li·ªáu

### Code:
```python
country_to_continent = {
    'Australia': 'Oceania',
    'Vietnam': 'Southeast Asia',
    'India': 'South Asia',
    # ... mapping ƒë·∫ßy ƒë·ªß
}

def get_continent(country):
    return country_to_continent.get(country, 'Other')

df_clean['Continent'] = df_clean['Country/Region'].apply(get_continent)
```

### Gi·∫£i th√≠ch:

#### 5.1. T·∫°o c·ªôt Continent t·ª´ Country/Region

**M·ª•c ƒë√≠ch:**
- T·∫°o feature m·ªõi t·ª´ feature hi·ªán c√≥ (Feature Engineering)
- Gi·∫£m s·ªë l∆∞·ª£ng gi√° tr·ªã duy nh·∫•t (47 qu·ªëc gia ‚Üí 13 khu v·ª±c)
- Gi√∫p model h·ªçc ƒë∆∞·ª£c pattern theo khu v·ª±c ƒë·ªãa l√Ω

**Mapping c√°c khu v·ª±c:**
- **Oceania**: Australia, Papua New Guinea
- **Southeast Asia**: Vietnam, Thailand, Cambodia, ...
- **South Asia**: India, Sri Lanka, Pakistan, Nepal
- **West Africa**: Ghana, Nigeria, Liberia, ...
- **Central Africa**: Cameroon, Congo (DRC), ...
- **East Africa**: Kenya, Uganda, Tanzania, ...
- **Northern Africa**: Egypt
- **North America**: USA (Florida), Mexico
- **Central America**: Costa Rica, Guatemala, Belize
- **Caribbean**: Cuba
- **South America**: Colombia, Venezuela
- **Western Asia**: Iran (historic)
- **Southern Africa**: South Africa
- **Other**: C√°c qu·ªëc gia kh√¥ng c√≥ trong mapping

#### 5.2. Ph√¢n b·ªë m·∫´u theo khu v·ª±c

**K·∫øt qu·∫£ ph√¢n b·ªë:**
```
Southeast Asia     229 m·∫´u (22.9%)
Oceania            151 m·∫´u (15.1%)
West Africa        147 m·∫´u (14.7%)
Central Africa     121 m·∫´u (12.1%)
South America       79 m·∫´u (7.9%)
Caribbean           77 m·∫´u (7.7%)
Central America     48 m·∫´u (4.8%)
South Asia          46 m·∫´u (4.6%)
North America       43 m·∫´u (4.3%)
East Africa         37 m·∫´u (3.7%)
Western Asia        11 m·∫´u (1.1%)
Southern Africa      8 m·∫´u (0.8%)
Northern Africa      3 m·∫´u (0.3%)
```

**Nh·∫≠n x√©t:**
- D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng gi·ªØa c√°c khu v·ª±c
- Southeast Asia v√† Oceania chi·∫øm t·ª∑ l·ªá cao nh·∫•t
- M·ªôt s·ªë khu v·ª±c c√≥ r·∫•t √≠t m·∫´u (Northern Africa ch·ªâ c√≥ 3 m·∫´u)

**·∫¢nh h∆∞·ªüng:**
- Model c√≥ th·ªÉ bias v·ªÅ c√°c khu v·ª±c c√≥ nhi·ªÅu d·ªØ li·ªáu
- C·∫ßn c√¢n nh·∫Øc khi d·ª± ƒëo√°n cho c√°c khu v·ª±c √≠t d·ªØ li·ªáu

---

## üìà Ph·∫ßn 6: Ph√¢n t√≠ch t∆∞∆°ng quan

### Code:
```python
correlation_matrix = df_clean[['Observed Length (m)', 'Observed Weight (kg)']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
```

### Gi·∫£i th√≠ch:

#### 6.1. Ma tr·∫≠n t∆∞∆°ng quan Pearson

**H·ªá s·ªë t∆∞∆°ng quan:**
- ƒêo m·ª©c ƒë·ªô quan h·ªá tuy·∫øn t√≠nh gi·ªØa hai bi·∫øn
- Gi√° tr·ªã t·ª´ -1 ƒë·∫øn +1:
  - **+1**: T∆∞∆°ng quan d∆∞∆°ng ho√†n h·∫£o
  - **0**: Kh√¥ng c√≥ t∆∞∆°ng quan
  - **-1**: T∆∞∆°ng quan √¢m ho√†n h·∫£o

**K·∫øt qu·∫£:**
```
Chi·ªÅu d√†i vs C√¢n n·∫∑ng: r = 0.8434
```

**Gi·∫£i th√≠ch:**
- H·ªá s·ªë t∆∞∆°ng quan **0.8434** cho th·∫•y c√≥ t∆∞∆°ng quan d∆∞∆°ng m·∫°nh
- C√° s·∫•u d√†i h∆°n th∆∞·ªùng n·∫∑ng h∆°n (ƒëi·ªÅu n√†y h·ª£p l√Ω v·ªÅ m·∫∑t sinh h·ªçc)
- T∆∞∆°ng quan cao c√≥ th·ªÉ g√¢y ƒëa c·ªông tuy·∫øn (multicollinearity)

#### 6.2. Heatmap tr·ª±c quan h√≥a

**Heatmap hi·ªÉn th·ªã:**
- M√†u ƒë·ªè: T∆∞∆°ng quan d∆∞∆°ng
- M√†u xanh: T∆∞∆°ng quan √¢m
- M√†u tr·∫Øng: Kh√¥ng t∆∞∆°ng quan

**·ª®ng d·ª•ng:**
- Ph√°t hi·ªán c√°c bi·∫øn c√≥ t∆∞∆°ng quan cao (c√≥ th·ªÉ lo·∫°i b·ªè m·ªôt trong hai)
- Hi·ªÉu m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn

**L∆∞u √Ω:**
- Trong tr∆∞·ªùng h·ª£p n√†y, gi·ªØ c·∫£ hai bi·∫øn v√¨:
  - C·∫£ hai ƒë·ªÅu c√≥ gi√° tr·ªã d·ª± ƒëo√°n
  - M·ªôt s·ªë thu·∫≠t to√°n c√≥ th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c ƒëa c·ªông tuy·∫øn

---

## üéØ Ph·∫ßn 7: Chu·∫©n b·ªã d·ªØ li·ªáu Train/Test

### Code:
```python
# M√£ h√≥a l·∫°i c√°c bi·∫øn
le_target = LabelEncoder()
df_clean['Conservation Status'] = le_target.fit_transform(df_clean['Conservation Status'])

age_mapping = {'Hatchling': 0, 'Juvenile': 1, 'Subadult': 2, 'Adult': 3}
df_clean['Age Class'] = df_clean['Age Class'].map(age_mapping)

cat_cols_to_encode = ['Sex', 'Country/Region', 'Habitat Type', 'Continent']
le_features = LabelEncoder()
for col in cat_cols_to_encode:
    df_clean[col] = le_features.fit_transform(df_clean[col])

# Chu·∫©n h√≥a l·∫°i
scaler = StandardScaler()
num_cols = ['Observed Length (m)', 'Observed Weight (kg)']
df_clean[num_cols] = scaler.fit_transform(df_clean[num_cols])

# Chia d·ªØ li·ªáu
X = df_clean.drop('Conservation Status', axis=1)
y = df_clean['Conservation Status']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
```

### Gi·∫£i th√≠ch:

#### 7.1. M√£ h√≥a l·∫°i d·ªØ li·ªáu

**T·∫°i sao m√£ h√≥a l·∫°i?**
- ƒê·∫£m b·∫£o t·∫•t c·∫£ bi·∫øn ƒë√£ ƒë∆∞·ª£c m√£ h√≥a ƒë√∫ng c√°ch
- Chu·∫©n b·ªã cho vi·ªác train model
- L∆∞u c√°c encoder ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y

**L∆∞u √Ω:**
- C·∫ßn fit encoder tr√™n to√†n b·ªô d·ªØ li·ªáu tr∆∞·ªõc khi chia train/test
- N·∫øu fit ri√™ng tr√™n train/test, c√≥ th·ªÉ g√¢y mismatch

#### 7.2. Chia d·ªØ li·ªáu Train/Test

**T·ª∑ l·ªá chia:**
- **Train: 70%** (700 m·∫´u)
- **Test: 30%** (300 m·∫´u)

**Stratified Split:**
- `stratify=y`: ƒê·∫£m b·∫£o t·ª∑ l·ªá c√°c l·ªõp trong train v√† test gi·ªëng nhau
- Tr√°nh tr∆∞·ªùng h·ª£p m·ªôt l·ªõp ch·ªâ c√≥ trong train ho·∫∑c test

**V√≠ d·ª• ph√¢n b·ªë:**
```
Train Set:
- Critically Endangered: 27.57%
- Data Deficient: 11.43%
- Endangered: 5.57%
- Least Concern: 38.43%
- Vulnerable: 17.00%

Test Set:
- Critically Endangered: 27.33%
- Data Deficient: 11.67%
- Endangered: 5.67%
- Least Concern: 38.33%
- Vulnerable: 17.00%
```

**random_state=42:**
- ƒê·∫£m b·∫£o k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p
- C√πng seed s·∫Ω cho c√πng k·∫øt qu·∫£ chia d·ªØ li·ªáu

---

## üìä Ph·∫ßn 8: Tr·ª±c quan h√≥a d·ªØ li·ªáu

### Code:
```python
# Ph√¢n b·ªë bi·∫øn m·ª•c ti√™u
sns.countplot(x=y, hue=y, palette='viridis')

# Scatter plot chi·ªÅu d√†i vs c√¢n n·∫∑ng
sns.scatterplot(data=df_clean, x='Observed Length (m)', 
                y='Observed Weight (kg)', hue='Age Class')

# Th·ªëng k√™ m√¥ t·∫£
df_clean[['Observed Length (m)', 'Observed Weight (kg)']].describe()
```

### Gi·∫£i th√≠ch:

#### 8.1. Ph√¢n b·ªë bi·∫øn m·ª•c ti√™u

**Countplot:**
- Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng m·∫´u cho m·ªói l·ªõp
- Gi√∫p ph√°t hi·ªán class imbalance

**K·∫øt qu·∫£:**
- Least Concern: 384 m·∫´u (38.4%) - L·ªõp ƒëa s·ªë
- Critically Endangered: 275 m·∫´u (27.5%)
- Vulnerable: 170 m·∫´u (17.0%)
- Data Deficient: 115 m·∫´u (11.5%)
- Endangered: 56 m·∫´u (5.6%) - L·ªõp thi·ªÉu s·ªë

**V·∫•n ƒë·ªÅ Class Imbalance:**
- Model c√≥ th·ªÉ bias v·ªÅ l·ªõp ƒëa s·ªë
- C·∫ßn c√¢n nh·∫Øc s·ª≠ d·ª•ng:
  - Class weights
  - SMOTE (oversampling)
  - Undersampling

#### 8.2. Scatter Plot

**M·ª•c ƒë√≠ch:**
- Tr·ª±c quan h√≥a m·ªëi quan h·ªá gi·ªØa chi·ªÅu d√†i v√† c√¢n n·∫∑ng
- Ph√¢n bi·ªát theo nh√≥m tu·ªïi (Age Class)

**Nh·∫≠n x√©t:**
- C√≥ xu h∆∞·ªõng tuy·∫øn t√≠nh gi·ªØa chi·ªÅu d√†i v√† c√¢n n·∫∑ng
- C√°c nh√≥m tu·ªïi kh√°c nhau c√≥ ph√¢n b·ªë kh√°c nhau
- Adult th∆∞·ªùng c√≥ chi·ªÅu d√†i v√† c√¢n n·∫∑ng l·ªõn h∆°n

#### 8.3. Th·ªëng k√™ m√¥ t·∫£

**C√°c ch·ªâ s·ªë:**
- **count**: S·ªë l∆∞·ª£ng m·∫´u
- **mean**: Gi√° tr·ªã trung b√¨nh
- **std**: ƒê·ªô l·ªách chu·∫©n
- **min, 25%, 50% (median), 75%, max**: C√°c t·ª© ph√¢n v·ªã

**Sau chu·∫©n h√≥a:**
- Mean ‚âà 0, Std ‚âà 1 (ƒë√∫ng nh∆∞ mong ƒë·ª£i)

---

## ü§ñ Ph·∫ßn 9: X√¢y d·ª±ng v√† ƒë√°nh gi√° m√¥ h√¨nh

### 9.1. Decision Tree (J48)

#### Code:
```python
j48_model = DecisionTreeClassifier(
    criterion='entropy',      # S·ª≠ d·ª•ng entropy (gi·ªëng J48)
    max_depth=4,              # Gi·ªõi h·∫°n ƒë·ªô s√¢u
    random_state=42
)
j48_model.fit(X_train, y_train)
```

#### Gi·∫£i th√≠ch:

**Decision Tree v·ªõi Entropy:**
- **criterion='entropy'**: S·ª≠ d·ª•ng Information Gain ƒë·ªÉ ch·ªçn feature t·ªët nh·∫•t
- **max_depth=4**: Gi·ªõi h·∫°n ƒë·ªô s√¢u ƒë·ªÉ tr√°nh overfitting

**K·∫øt qu·∫£:**
- Accuracy tr√™n Test Set: **72.67%**
- Precision, Recall, F1-Score kh√°c nhau cho t·ª´ng l·ªõp

**Ph√¢n t√≠ch:**
- Critically Endangered: Recall = 1.00 (t√¨m ƒë∆∞·ª£c t·∫•t c·∫£), nh∆∞ng Precision = 0.52 (nhi·ªÅu d·ª± ƒëo√°n sai)
- Least Concern: Precision = 0.97, Recall = 0.68 (d·ª± ƒëo√°n ƒë√∫ng nh∆∞ng b·ªè s√≥t m·ªôt s·ªë)
- Data Deficient: Precision = 0.95 nh∆∞ng Recall = 0.51 (d·ª± ƒëo√°n ƒë√∫ng nh∆∞ng b·ªè s√≥t nhi·ªÅu)

**Visualization:**
- V·∫Ω c√¢y quy·∫øt ƒë·ªãnh ƒë·ªÉ hi·ªÉu logic c·ªßa model
- Export rules d·∫°ng text ƒë·ªÉ gi·∫£i th√≠ch

### 9.2. Decision Tree v·ªõi max_depth=5

#### Code:
```python
dt_model = DecisionTreeClassifier(
    criterion='entropy',
    max_depth=5,
    random_state=42
)
```

#### K·∫øt qu·∫£:
- **Train Accuracy: 79.57%**
- **Test Accuracy: 80.00%**

**So s√°nh v·ªõi max_depth=4:**
- TƒÉng ƒë·ªô s√¢u ‚Üí tƒÉng ƒë·ªô ch√≠nh x√°c
- Gap gi·ªØa train v√† test nh·ªè ‚Üí kh√¥ng b·ªã overfitting nhi·ªÅu

### 9.3. K-Means Clustering

#### Code:
```python
kmeans = KMeans(n_clusters=5, random_state=42)  # 5 clusters = 5 classes
kmeans.fit(X_train)
```

#### Gi·∫£i th√≠ch:

**K-Means:**
- Ph√¢n c·ª•m d·ªØ li·ªáu th√†nh 5 nh√≥m (t∆∞∆°ng ·ª©ng v·ªõi 5 l·ªõp)
- Kh√¥ng ph·∫£i thu·∫≠t to√°n ph√¢n lo·∫°i, nh∆∞ng c√≥ th·ªÉ d√πng ƒë·ªÉ ph√¢n c·ª•m

**Silhouette Score:**
- ƒêo ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m
- Gi√° tr·ªã t·ª´ -1 ƒë·∫øn +1:
  - **+1**: Ph√¢n c·ª•m t·ªët
  - **0**: Ch·ªìng ch√©o gi·ªØa c√°c c·ª•m
  - **-1**: Ph√¢n c·ª•m sai

**K·∫øt qu·∫£:**
- Silhouette Score (Train): **0.4019**
- Silhouette Score (Test): **0.4084**

**Nh·∫≠n x√©t:**
- Score trung b√¨nh (~0.40) cho th·∫•y ph√¢n c·ª•m ch·∫•p nh·∫≠n ƒë∆∞·ª£c
- Kh√¥ng t·ªët b·∫±ng Decision Tree cho b√†i to√°n ph√¢n lo·∫°i

### 9.4. Naive Bayes

#### Code:
```python
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
```

#### Gi·∫£i th√≠ch:

**Gaussian Naive Bayes:**
- Gi·∫£ ƒë·ªãnh c√°c features ƒë·ªôc l·∫≠p v·ªõi nhau (naive assumption)
- S·ª≠ d·ª•ng ph√¢n ph·ªëi chu·∫©n (Gaussian) cho c√°c bi·∫øn s·ªë

**K·∫øt qu·∫£:**
- **Train Accuracy: 38.71%**
- **Test Accuracy: 35.33%**

**Ph√¢n t√≠ch:**
- Hi·ªáu su·∫•t th·∫•p nh·∫•t trong c√°c m√¥ h√¨nh
- C√≥ th·ªÉ do:
  - Gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p kh√¥ng ph√π h·ª£p (c√°c bi·∫øn c√≥ t∆∞∆°ng quan)
  - Ph√¢n ph·ªëi kh√¥ng chu·∫©n sau khi chu·∫©n h√≥a
  - D·ªØ li·ªáu kh√¥ng ph√π h·ª£p v·ªõi gi·∫£ ƒë·ªãnh c·ªßa Naive Bayes

---

## üìä Ph·∫ßn 10: So s√°nh c√°c m√¥ h√¨nh

### Code:
```python
models = ['Decision Tree (J48)', 'Naive Bayes']
accuracies = [80.00, 35.33]  # Test accuracy
plt.bar(models, accuracies)
```

### K·∫øt qu·∫£ so s√°nh:

| M√¥ h√¨nh | Train Accuracy | Test Accuracy | Nh·∫≠n x√©t |
|---------|---------------|---------------|----------|
| **Decision Tree (J48)** | 79.57% | 80.00% | ‚úÖ T·ªët nh·∫•t, kh√¥ng overfitting |
| **Naive Bayes** | 38.71% | 35.33% | ‚ùå Hi·ªáu su·∫•t th·∫•p |
| **K-Means** | - | Silhouette: 40.84% | ‚ö†Ô∏è Kh√¥ng ph√π h·ª£p cho ph√¢n lo·∫°i |

### K·∫øt lu·∫≠n:

1. **Decision Tree (J48) l√† l·ª±a ch·ªçn t·ªët nh·∫•t:**
   - ƒê·ªô ch√≠nh x√°c cao (80%)
   - Kh√¥ng b·ªã overfitting (train ‚âà test)
   - D·ªÖ gi·∫£i th√≠ch (c√¢y quy·∫øt ƒë·ªãnh)

2. **Naive Bayes kh√¥ng ph√π h·ª£p:**
   - Gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p kh√¥ng ƒë√∫ng v·ªõi d·ªØ li·ªáu
   - C·∫ßn d·ªØ li·ªáu ph√π h·ª£p h∆°n v·ªõi gi·∫£ ƒë·ªãnh

3. **K-Means:**
   - Ph√π h·ª£p cho clustering, kh√¥ng ph·∫£i classification
   - C√≥ th·ªÉ d√πng ƒë·ªÉ ph√¢n c·ª•m d·ªØ li·ªáu tr∆∞·ªõc khi ph√¢n lo·∫°i

---

## üéÆ Ph·∫ßn 11: Demo ·ª©ng d·ª•ng

### Code:
```python
def predict_crocodile_status(length, weight, habitat_code):
    # T·∫°o input t·ª´ th√¥ng tin ƒë·∫ßu v√†o
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    # D·ª± ƒëo√°n b·∫±ng model
    return prediction

# K·ªãch b·∫£n 1: C√° s·∫•u con
result1 = predict_crocodile_status(0.8, 5.0, 1)
# ‚Üí "CRITICALLY ENDANGERED - C·∫¶N B·∫¢O V·ªÜ!"

# K·ªãch b·∫£n 2: C√° s·∫•u tr∆∞·ªüng th√†nh
result2 = predict_crocodile_status(4.5, 300.0, 2)
# ‚Üí "Least Concern"
```

### Gi·∫£i th√≠ch:

#### 11.1. H√†m d·ª± ƒëo√°n

**Input:**
- `length`: Chi·ªÅu d√†i (m)
- `weight`: C√¢n n·∫∑ng (kg)
- `habitat_code`: M√£ m√¥i tr∆∞·ªùng s·ªëng

**X·ª≠ l√Ω:**
1. T·∫°o DataFrame t·ª´ input
2. Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë (s·ª≠ d·ª•ng scaler ƒë√£ fit)
3. M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i
4. D·ª± ƒëo√°n b·∫±ng model
5. Gi·∫£i m√£ nh√£n v·ªÅ t√™n g·ªëc

**Output:**
- T√¨nh tr·∫°ng b·∫£o t·ªìn d·ª± ƒëo√°n

#### 11.2. K·ªãch b·∫£n ·ª©ng d·ª•ng

**K·ªãch b·∫£n 1: C√° s·∫•u con**
- Chi·ªÅu d√†i: 0.8m (r·∫•t nh·ªè)
- C√¢n n·∫∑ng: 5kg
- ‚Üí D·ª± ƒëo√°n: **Critically Endangered**
- ‚Üí H√†nh ƒë·ªông: C·∫ßn b·∫£o v·ªá ngay l·∫≠p t·ª©c

**K·ªãch b·∫£n 2: C√° s·∫•u tr∆∞·ªüng th√†nh**
- Chi·ªÅu d√†i: 4.5m (l·ªõn)
- C√¢n n·∫∑ng: 300kg
- ‚Üí D·ª± ƒëo√°n: **Least Concern**
- ‚Üí H√†nh ƒë·ªông: T√¨nh tr·∫°ng ·ªïn ƒë·ªãnh

#### 11.3. ·ª®ng d·ª•ng th·ª±c t·∫ø

**C√°c ·ª©ng d·ª•ng c√≥ th·ªÉ:**
1. **H·ªá th·ªëng gi√°m s√°t t·ª± ƒë·ªông:**
   - Camera t·ª± ƒë·ªông ƒëo k√≠ch th∆∞·ªõc
   - H·ªá th·ªëng c·∫£nh b√°o khi ph√°t hi·ªán c√° s·∫•u nguy c·∫•p

2. **·ª®ng d·ª•ng di ƒë·ªông:**
   - Ki·ªÉm l√¢m nh·∫≠p th√¥ng tin quan s√°t
   - Nh·∫≠n c·∫£nh b√°o v√† khuy·∫øn ngh·ªã

3. **Ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn:**
   - Ph√¢n t√≠ch xu h∆∞·ªõng b·∫£o t·ªìn
   - D·ª± ƒëo√°n t√¨nh tr·∫°ng trong t∆∞∆°ng lai

---

## üìù T·ªïng k·∫øt

### Quy tr√¨nh ƒë√£ th·ª±c hi·ªán:

1. ‚úÖ **ƒê·ªçc v√† ki·ªÉm tra d·ªØ li·ªáu**
2. ‚úÖ **L√†m s·∫°ch d·ªØ li·ªáu** (x·ª≠ l√Ω missing values, lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn)
3. ‚úÖ **X·ª≠ l√Ω outlier** (IQR capping)
4. ‚úÖ **M√£ h√≥a v√† chu·∫©n h√≥a** (LabelEncoder, StandardScaler)
5. ‚úÖ **Feature Engineering** (t·∫°o c·ªôt Continent)
6. ‚úÖ **Ph√¢n t√≠ch t∆∞∆°ng quan**
7. ‚úÖ **Chia d·ªØ li·ªáu train/test** (stratified split)
8. ‚úÖ **Tr·ª±c quan h√≥a d·ªØ li·ªáu**
9. ‚úÖ **X√¢y d·ª±ng m√¥ h√¨nh** (Decision Tree, Naive Bayes, K-Means)
10. ‚úÖ **ƒê√°nh gi√° v√† so s√°nh m√¥ h√¨nh**
11. ‚úÖ **Demo ·ª©ng d·ª•ng**

### K·∫øt qu·∫£:

- **M√¥ h√¨nh t·ªët nh·∫•t**: Decision Tree (J48) v·ªõi ƒë·ªô ch√≠nh x√°c **80%**
- **D·ªØ li·ªáu sau x·ª≠ l√Ω**: S·∫°ch, chu·∫©n h√≥a, s·∫µn s√†ng cho machine learning
- **·ª®ng d·ª•ng**: C√≥ th·ªÉ t√≠ch h·ª£p v√†o h·ªá th·ªëng th·ª±c t·∫ø

### L∆∞u √Ω:

- D·ªØ li·ªáu c√≥ class imbalance ‚Üí c√¢n nh·∫Øc s·ª≠ d·ª•ng class weights
- M·ªôt s·ªë khu v·ª±c c√≥ √≠t d·ªØ li·ªáu ‚Üí c·∫ßn th√™m d·ªØ li·ªáu ho·∫∑c x·ª≠ l√Ω ƒë·∫∑c bi·ªát
- Model c√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng c√°ch:
  - TƒÉng s·ªë l∆∞·ª£ng d·ªØ li·ªáu
  - Feature engineering t·ªët h∆°n
  - Th·ª≠ c√°c thu·∫≠t to√°n kh√°c (Random Forest, XGBoost)

---

**T√°c gi·∫£:** B√πi Ho√†n Duy - Nguy·ªÖn Tu·∫•n Ki·ªát - V√µ Minh Th·∫Øng - Nguy·ªÖn B√¨nh Ti·∫øn

**Ng√†y t·∫°o:** 2025


